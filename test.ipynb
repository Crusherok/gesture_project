{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5d1901-0ae9-4fb2-8e0b-94856de2b878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting live gesture recognition. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# (uncomment the above line if you don't want OpenCV debug prints)\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MODEL_PATH      = \"models/sequence_gesture_model3.keras\"\n",
    "ENCODER_PATH    = \"models/sequence_label_encoder3.pkl\"\n",
    "SEQUENCE_LENGTH = 120\n",
    "NUM_FEATURES    = 63\n",
    "\n",
    "# ‚îÄ‚îÄ LOAD MODEL & LABEL ENCODER ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "le    = joblib.load(ENCODER_PATH)\n",
    "\n",
    "# ‚îÄ‚îÄ SETUP MEDIAPIPE & OPENCV ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "mp_hands   = mp.solutions.hands\n",
    "mp_draw    = mp.solutions.drawing_utils\n",
    "hands      = mp_hands.Hands(static_image_mode=False,\n",
    "                           max_num_hands=1,\n",
    "                           min_detection_confidence=0.7,\n",
    "                           min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []       # rolling buffer of last 120 feature vectors\n",
    "pred_text = \"\"      # last predicted label\n",
    "\n",
    "print(\"Starting live gesture recognition. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb   = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    res   = hands.process(rgb)\n",
    "\n",
    "    if res.multi_hand_landmarks:\n",
    "        for handLms in res.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # extract 63-dim feature vector\n",
    "            feats = []\n",
    "            for lm in handLms.landmark:\n",
    "                feats += [lm.x, lm.y, lm.z]\n",
    "            sequence.append(feats)\n",
    "\n",
    "            # keep only the last SEQUENCE_LENGTH frames\n",
    "            if len(sequence) > SEQUENCE_LENGTH:\n",
    "                sequence.pop(0)\n",
    "\n",
    "            # when we have enough frames, predict\n",
    "            if len(sequence) == SEQUENCE_LENGTH:\n",
    "                X = np.array(sequence, dtype=np.float32).reshape(1, SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "                probs = model.predict(X, verbose=0)[0]\n",
    "                idx   = np.argmax(probs)\n",
    "                pred_text = le.inverse_transform([idx])[0]\n",
    "\n",
    "    # overlay prediction\n",
    "    cv2.putText(frame, f\"Gesture: {pred_text}\", (10, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Live Gesture Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b25134-bbd2-446b-9345-6a88e91d65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "MODEL_PATH      = \"models/static_gesture_model.keras\"\n",
    "ENCODER_PATH    = \"models/static_label_encoder.pkl\"\n",
    "NUM_FEATURES    = 63\n",
    "\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "le    = joblib.load(ENCODER_PATH)\n",
    "\n",
    "mp_hands   = mp.solutions.hands\n",
    "mp_draw    = mp.solutions.drawing_utils\n",
    "hands      = mp_hands.Hands(static_image_mode=False,\n",
    "                           max_num_hands=1,\n",
    "                           min_detection_confidence=0.7,\n",
    "                           min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "pred_text = \"\"\n",
    "\n",
    "print(\"Starting live static gesture recognition. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb   = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    res   = hands.process(rgb)\n",
    "\n",
    "    if res.multi_hand_landmarks:\n",
    "        for handLms in res.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            feats = []\n",
    "            for lm in handLms.landmark:\n",
    "                feats += [lm.x, lm.y, lm.z]\n",
    "\n",
    "            # Predict directly on the 63 features\n",
    "            X = np.array(feats, dtype=np.float32).reshape(1, NUM_FEATURES)\n",
    "            probs = model.predict(X, verbose=0)[0]\n",
    "            idx   = np.argmax(probs)\n",
    "            pred_text = le.inverse_transform([idx])[0]\n",
    "\n",
    "    # overlay prediction\n",
    "    cv2.putText(frame, f\"Gesture: {pred_text}\", (10, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Live Static Gesture Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "üñê Gesture Env",
   "language": "python",
   "name": "gesture_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
